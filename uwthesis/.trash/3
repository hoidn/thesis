\chapter{Real-time analysis tools for the LCLS}

A significant drawback of current XFELs, compared to synchrotron light sources, is that they are capable of providing photons to only one endstation at a time. As a result they are vastly oversubscribed, and beamtime is granted in small allocations through a highly competitive selection process. Experimental teams therefore have strong incentives to make the most efficient possible use of beam time. Focusing on the specific case of the LCLS, the high power of the XFEL source (and its 120 Hz repition rate) facilitates rapid completion of experiments by making very rapid data collection rates possible . However, fully taking advantage of high data throughput is a problem unto itself, as experiments cannot be fully scripted in advance; it is in practice necessary to make rapid evaluations based on measurement of beam conditions (which, in two-color mode, can be strongly variable), the statistical quality of incoming data, and tentative physical interpretations in the incoming data. This feedback often guides importand decisions, such as beam tuning and the motion of samples and detectors. 

With the goal of addressing this problem we have developed a softare package for real-time analysis and visualization of data in XFEL experiments at the LCLS. The software is implemented using Photon Science Analysis (psana), the internal data analysis framework at the LCLS, and can be run in distributed fashion over hundreds of cores. (cite Damiani et al. 2016). It attempts to enable a more effective analysis workflow than currently available to LCLS users, excluding those doing specialized experiments of well-established types for which there already exist tailored software packages (such as Cheetah for serial femtosecond crystallography). (cite Barty et al).

This chapter describes two user interfaces that have been developed on top of a common backend:

\begin{itemize}
\item{A Python API that that provides high-level analysis and visualization functions that directly implement common analysis operations, and can serve as building blocks more complex custom onoes. The API is optimized for use through the Jupyter notebook, and it leverages the rich interactive plotting features available in that environment.}
\item{An in-browser web app providing a more user-friendly graphical interface, in exchange for less flexibility; it currently supports XRD and XES analysis. This interface was independently developed by Ryan Valenza.}
\end{itemize}


\subsection{Integration of Logging and Analysis}
The psana API associates every LCLS pulse (referred to as an event) with two integers, a run number and event number. (cite Damiani et al. 2016) A run contains a consecutive sequence of events; the maximum number of events in a run is determined by a 17 bit 360 Hz `fiducial' counter that the LCLS timing system distributes to each detector. The association of run/event number combinations to LCLS pulses is in practice further constrained by endstation-specific software and instrumental details: at the Matter of Extreme Conditions beamline (MEC), for example, performing a two-dimensional sample raster involves interruptions between horizontal rows, which in turn requires the run number to be incremented at the beggining of each row. 

% awkward
Because the LCLS DAQ system does not allow the association of events with user-provided metadata, LCLS users must maintain separate experimental logs in which, for each run number and range of event numbers, they record information related to sample type, beam conditions, and other relevant experimental parameters.  In the vast majority of cases, users' analysis scripts directly expose psana's data access API, requiring them to explicitly specify datasets in terms of lists of run numbers. To do this the user must manually look up and transcribe information from the experimental logbook, an operation that becomes time-consuming (and potentially error-prone) when frequently repeated, as it usually must be.

In response, we've made a step to unify the workflows for experimental logging and analysis. We've defined a simple query language with which the user can construct datasets defined by matches to metadata attributes recorded in the experimental logbook. The implementation is described in Fig. (reference figure); briefly, it consists of a daemon (i.e. application component) that acesses data from standard-formatted Google Drive spreadsheets tied to users' personal Google accounts. This daemon parses spreadsheet data into a graph structure associating run numbers with metadata column values and constructs datasets (i.e., sets of run numbers) by parsing user-provided queries on those column values. 

\subsection{Interactive Distributed Computing}
A second feature of the software is the simplified fashion in which it leverages the LCLS computing cluster. To do real-time analysis during beam runs it is typically necessary to scale one's workload over tensor hundreds of CPU cores. This is typically done with a batch processing workflow, where command lines for launching parallel analysis scripts are submitted to the LCLS's Platform Load Sharing Facility (LSF) (cites), which schedules them for execution on nodes of the cluster. Once a batch job is complete, users typically run a second (non-distributed) program to load and visualize the output data. The time that elapses between submission of a batch job and when it begins running is typically on the order 10 seconds or more, assuming an empty job queue. The separation between the steps of submitting an analysis batch job and loading and viewing its results introduces a delay as well, because the user must manually intervene at two points in the analysis pipeline. These two factors give a batch-processing-based workflow significant overhead. 

Our software package offers a significant improvement in this respect: the backend of our analysis API distributes calculations over the LCLS cluster transparently, with no need for the user to submit batch jobs or otherwise steer the parallel computation in any way. To implement this we use the parallel computing library Dask (cite Rocklin). Dask consists of several components, one of which is a dynamic task scheduler optimized for computational workloads. Its valuable features, in our context, are its ability to dispatch work to a pool of worker processes in intelligent fashion (with awareness of task dependencies and data locality) and with flexibility and fault tolerance (allowing for the dynamic addition, as well as removal, of worker processes). From the point of view of interactive computation, its most important feature is that it implements the same synchronous semantics as an equivalent single-threaded program. 


an automated approach 

The batch processing workflow has intrinsic overhead related 
This ability to define datasets in this way is the first of two major features

Google Drive-based notebook template that, in combination with a 


. This is typically done using a cloud-hosted spreadsheet (such as on Google Drive) to record 
 

The LCLS indexes every event by For all detectors the LCLS that generate data at the rate of 
In data collection at the LCLS it is typical to maintain a logbook associating various each 

metadata (such as sample type,


The software contains two components: first, a query language 
A common occurence in 

%\begin{itemize}


